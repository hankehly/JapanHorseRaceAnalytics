{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0803387-a429-46e9-8804-1d116d719ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from loguru import logger\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "\n",
    "from jhra.jrdb import create_pyspark_schema, load_schema, parse_line\n",
    "from jhra.settings import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5f359",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8096d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    # Taken from http://www.jrdb.com/member/dataindex.html\n",
    "    # Comment out the ones you don't want to download.\n",
    "    # Downloading all of them will take about ?\n",
    "    \"http://www.jrdb.com/member/datazip/Kab/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Bac/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Kyi/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Ukc/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Oz/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Oz/index2.html\",  # OW data\n",
    "    \"http://www.jrdb.com/member/datazip/Ou/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Ot/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Ov/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Cyb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Cha/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Sed/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Skb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Tyb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Hjc/index.html\",\n",
    "]\n",
    "\n",
    "# for webpage_url in urls:\n",
    "#     download_and_extract_files(\n",
    "#         webpage_url, username, password, JRDB_DATA_DIR, start_date=datetime.date(2023, 12, 1)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d318",
   "metadata": {},
   "source": [
    "# Full Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionURL\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionPassword\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionDriverName\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.schema.verification\n",
      "Warning: Ignoring non-Spark config property: datanucleus.schema.autoCreateTables\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionUserName\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/18 02:43:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 02:43:25.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mDropping schema jhra_raw\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/18 02:43:27 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/10/18 02:43:27 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 02:43:27.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mCreating schema jhra_raw\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/18 02:43:27 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/10/18 02:43:27 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.2\n",
      "25/10/18 02:43:27 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/10/18 02:43:27 WARN TxnHandler: Cannot perform cleanup since metastore table does not exist\n",
      "25/10/18 02:43:27 WARN ObjectStore: Failed to get database jhra_raw, returning NoSuchObjectException\n",
      "25/10/18 02:43:27 WARN ObjectStore: Failed to get database jhra_raw, returning NoSuchObjectException\n",
      "25/10/18 02:43:27 WARN ObjectStore: Failed to get database jhra_raw, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 02:43:28.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset KAB from /workspace/data/jrdb/KAB[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/18 02:43:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 02:43:38.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset BAC from /workspace/data/jrdb/BAC[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/18 02:43:38 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/10/18 02:43:38 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/10/18 02:43:38 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/10/18 02:43:38 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 02:43:43.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset KYI from /workspace/data/jrdb/KYI[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:44:17.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset UKC from /workspace/data/jrdb/UKC[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:44:27.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset OZ from /workspace/data/jrdb/OZ[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:44:33.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset OW from /workspace/data/jrdb/OW[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:44:38.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset OU from /workspace/data/jrdb/OU[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:44:44.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset OT from /workspace/data/jrdb/OT[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:44:55.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset OV from /workspace/data/jrdb/OV[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:45:37.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset CYB from /workspace/data/jrdb/CYB[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:45:46.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset CHA from /workspace/data/jrdb/CHA[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:45:52.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset SKB from /workspace/data/jrdb/SKB[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:46:07.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset SRB from /workspace/data/jrdb/SRB[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:46:12.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset HJC from /workspace/data/jrdb/HJC[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:46:16.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset SED from /workspace/data/jrdb/SED[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "\u001b[32m2025-10-18 02:46:37.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mLoading JRDB dataset TYB from /workspace/data/jrdb/TYB[0-9][0-9][0-9][0-9][0-9][0-9].txt\u001b[0m\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "| jhra_raw|raw_jrdb__bac|      false|\n",
      "| jhra_raw|raw_jrdb__cha|      false|\n",
      "| jhra_raw|raw_jrdb__cyb|      false|\n",
      "| jhra_raw|raw_jrdb__hjc|      false|\n",
      "| jhra_raw|raw_jrdb__kab|      false|\n",
      "| jhra_raw|raw_jrdb__kyi|      false|\n",
      "| jhra_raw| raw_jrdb__ot|      false|\n",
      "| jhra_raw| raw_jrdb__ou|      false|\n",
      "| jhra_raw| raw_jrdb__ov|      false|\n",
      "| jhra_raw| raw_jrdb__ow|      false|\n",
      "| jhra_raw| raw_jrdb__oz|      false|\n",
      "| jhra_raw|raw_jrdb__sed|      false|\n",
      "| jhra_raw|raw_jrdb__skb|      false|\n",
      "| jhra_raw|raw_jrdb__srb|      false|\n",
      "| jhra_raw|raw_jrdb__tyb|      false|\n",
      "| jhra_raw|raw_jrdb__ukc|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .config(\"spark.sql.warehouse.dir\", settings.SPARK_WAREHOUSE_DIR)\n",
    "    .config(\"javax.jdo.option.ConnectionURL\", settings.HIVE_METASTORE_URL)\n",
    "    .config(\"javax.jdo.option.ConnectionDriverName\", \"org.postgresql.Driver\")\n",
    "    .config(\"javax.jdo.option.ConnectionUserName\", \"admin\")\n",
    "    .config(\"javax.jdo.option.ConnectionPassword\", \"admin\")\n",
    "    .config(\"datanucleus.schema.autoCreateTables\", \"true\")\n",
    "    .config(\"hive.metastore.schema.verification\", \"false\")\n",
    "    .config(\"spark.driver.extraClassPath\", settings.POSTGRES_JDBC_JAR)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Drop schema if exists\n",
    "logger.info(f\"Dropping schema {settings.SCHEMA_NAME}\")\n",
    "spark.sql(f\"DROP SCHEMA IF EXISTS {settings.SCHEMA_NAME} CASCADE\")\n",
    "\n",
    "# Create schema\n",
    "logger.info(f\"Creating schema {settings.SCHEMA_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA {settings.SCHEMA_NAME}\")\n",
    "\n",
    "datasets = [\n",
    "    # fmt: off\n",
    "    \"KAB\", \"BAC\", \"KYI\", \"UKC\",\n",
    "    \"OZ\",  \"OW\",  \"OU\",  \"OT\",\n",
    "    \"OV\",  \"CYB\", \"CHA\", \"SKB\",\n",
    "    \"SRB\", \"HJC\", \"SED\", \"TYB\",\n",
    "    # fmt: on\n",
    "]\n",
    "\n",
    "for ds in datasets:\n",
    "    data_path = (settings.JRDB_DATA_DIR / f\"{ds}[0-9][0-9][0-9][0-9][0-9][0-9].txt\").as_posix()\n",
    "    table_name = f\"{settings.SCHEMA_NAME}.raw_jrdb__{ds.lower()}\"\n",
    "    jrdb_fields = load_schema(settings.SCHEMAS_DIR / f\"{ds}.yaml\")\n",
    "    parsed_struct = create_pyspark_schema(jrdb_fields)\n",
    "\n",
    "    logger.info(f\"Loading JRDB dataset {ds} from {data_path}\")\n",
    "\n",
    "    base_df = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "        .load(data_path)\n",
    "        .withColumn(\"file_name\", F.element_at(F.split(F.col(\"path\"), \"/\"), -1))\n",
    "        .withColumn(\"sha256\", F.sha2(F.col(\"content\"), 256))\n",
    "        .select(\"file_name\", \"sha256\", \"content\")\n",
    "    )\n",
    "\n",
    "    rdd = base_df.rdd.flatMap(\n",
    "        lambda r: (\n",
    "            Row(file_name=r.file_name, sha256=r.sha256, **parse_line(line, jrdb_fields).asDict())\n",
    "            for line in r.content.splitlines()\n",
    "            if line  # skip empty lines\n",
    "        )\n",
    "    )\n",
    "\n",
    "    final_schema = StructType([\n",
    "        StructField(\"file_name\", StringType(), False),\n",
    "        StructField(\"sha256\", StringType(), False),\n",
    "        *parsed_struct.fields\n",
    "    ])\n",
    "\n",
    "    parsed_df = spark.createDataFrame(rdd, schema=final_schema)\n",
    "    parsed_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "spark.sql(f\"SHOW TABLES IN {settings.SCHEMA_NAME}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7857759",
   "metadata": {},
   "source": [
    "# Convert codes to CSV format\n",
    "\n",
    "Copy and paste text from the code web pages into the following block, run cell, and save as a CSV file in the `seeds` directory.\n",
    "\n",
    "* [ＪＲＤＢデータコード表](http://www.jrdb.com/program/jrdb_code.txt)\n",
    "* [脚元コード表（2017.02.20）](http://www.jrdb.com/program/ashimoto_code.txt)\n",
    "* [馬具コード表（2017.07.02）](http://www.jrdb.com/program/bagu_code.txt)\n",
    "* [特記コード表（2008.02.23）](http://www.jrdb.com/program/tokki_code.txt)\n",
    "* [系統コード表（2003.05.15）](http://www.jrdb.com/program/keito_code.txt)\n",
    "* [調教コースコード表（2009.10.09）](http://www.jrdb.com/program/cyokyo_course_code.txt)\n",
    "* [追い状態コード表（2008.09.28）](http://www.jrdb.com/program/oi_code.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"\n",
    "01      流す\n",
    "02      余力あり\n",
    "03      終い抑え\n",
    "04      一杯\n",
    "05      バテる\n",
    "06      伸びる\n",
    "07      テンのみ\n",
    "08      鋭く伸び\n",
    "09      強目\n",
    "10      終い重点\n",
    "11      ８分追い\n",
    "12      追って伸\n",
    "13      向正面\n",
    "14      ゲート\n",
    "15      障害練習\n",
    "16      中間軽め\n",
    "17      キリ\n",
    "21      引っ張る\n",
    "22      掛かる\n",
    "23      掛リバテ\n",
    "24      テン掛る\n",
    "25      掛り一杯\n",
    "26      ササル\n",
    "27      ヨレル\n",
    "28      バカつく\n",
    "29      手間取る\n",
    "99      その他\n",
    "\"\"\"\n",
    "\n",
    "result = []\n",
    "for line in code_text.strip().splitlines():\n",
    "    result.append(line.strip().split())\n",
    "\n",
    "print(pd.DataFrame(result).to_csv(index=False, header=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "japanhorseraceanalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
