{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0803387-a429-46e9-8804-1d116d719ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import functools\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from jhra.data.data_schema import create_pyspark_schema, load_schema, parse_line\n",
    "\n",
    "# from jhra.data.file_downloader import download_and_extract_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5f359",
   "metadata": {},
   "source": [
    "# Download files from JRDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"JRDB_USERNAME\")\n",
    "password = os.getenv(\"JRDB_PASSWORD\")\n",
    "download_dir = \"path/to/data/jrdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8096d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset_urls = [\n",
    "    # Taken from http://www.jrdb.com/member/dataindex.html\n",
    "    # Comment out the ones you don't want to download.\n",
    "    # Downloading all of them will take about ?\n",
    "    \"http://www.jrdb.com/member/datazip/Kab/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Bac/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Kyi/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Ukc/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Oz/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Oz/index2.html\",  # OW data\n",
    "    # \"http://www.jrdb.com/member/datazip/Ou/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Ot/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Ov/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Cyb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Cha/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Sed/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Skb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Tyb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Hjc/index.html\",\n",
    "]\n",
    "\n",
    "for webpage_url in target_dataset_urls:\n",
    "    download_and_extract_files(\n",
    "        webpage_url, username, password, download_dir, start_date=datetime.date(2023, 12, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d318",
   "metadata": {},
   "source": [
    "# Ingest data into warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05873ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionURL\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionPassword\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionDriverName\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.schema.verification\n",
      "Warning: Ignoring non-Spark config property: datanucleus.schema.autoCreateTables\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionUserName\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/13 12:01:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(\"spark.sql.warehouse.dir\", os.environ.get(\"SPARK_WAREHOUSE_DIR\"))\n",
    "    .config(\"javax.jdo.option.ConnectionURL\", \"jdbc:postgresql://postgres/metastore\")\n",
    "    .config(\"javax.jdo.option.ConnectionDriverName\", \"org.postgresql.Driver\")\n",
    "    .config(\"javax.jdo.option.ConnectionUserName\", \"admin\")\n",
    "    .config(\"javax.jdo.option.ConnectionPassword\", \"admin\")\n",
    "\n",
    "    # .config(\"spark.hadoop.datanucleus.autoCreateTables\", \"true\")\n",
    "    # .config(\"spark.hadoop.datanucleus.schema.autoCreateTables\", \"true\")\n",
    "    .config(\"datanucleus.schema.autoCreateTables\", \"true\")\n",
    "\n",
    "    .config(\"hive.metastore.schema.verification\", \"false\")\n",
    "    .config(\"spark.driver.extraClassPath\", os.environ.get(\"POSTGRES_JDBC_JAR\"))\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------+------------------------------------------------------+-------------+\n",
      "|key                    |value                |meaning                                               |Since version|\n",
      "+-----------------------+---------------------+------------------------------------------------------+-------------+\n",
      "|spark.sql.warehouse.dir|file:/spark-warehouse|The default location for managed databases and tables.|2.0.0        |\n",
      "+-----------------------+---------------------+------------------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"set -v\").filter(\"key rlike 'warehouse.dir$'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|jrdb_staging|\n",
      "| raw_staging|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW SCHEMAS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014d2e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/13 12:02:28 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/10/13 12:02:28 WARN FileUtils: File file:/spark-warehouse/raw.db/raw_jrdb__bac does not exist; Force to delete it.\n",
      "25/10/13 12:02:28 ERROR FileUtils: Failed to delete file:/spark-warehouse/raw.db/raw_jrdb__bac\n",
      "25/10/13 12:02:28 WARN FileUtils: File file:/spark-warehouse/raw.db does not exist; Force to delete it.\n",
      "25/10/13 12:02:28 ERROR FileUtils: Failed to delete file:/spark-warehouse/raw.db\n",
      "25/10/13 12:02:28 WARN TxnHandler: Cannot perform cleanup since metastore table does not exist\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP SCHEMA IF EXISTS raw CASCADE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/13 12:03:07 WARN ObjectStore: Failed to get database raw, returning NoSuchObjectException\n",
      "25/10/13 12:03:07 WARN ObjectStore: Failed to get database raw, returning NoSuchObjectException\n",
      "25/10/13 12:03:07 WARN ObjectStore: Failed to get database raw, returning NoSuchObjectException\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Necessary\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                         (0 + 8) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/13 12:03:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/13 12:03:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/10/13 12:03:17 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/10/13 12:03:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/10/13 12:03:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "dataset = \"BAC\"\n",
    "dbtable = f\"{schema_name}.raw_jrdb__{dataset.lower()}\"\n",
    "data_path = str(Path(\"/workspace/data/jrdb\").joinpath(f\"{dataset}*.txt\"))\n",
    "dataset_schema = load_schema(\"schemas/BAC.yaml\")\n",
    "column_names = [field.name for field in dataset_schema]\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .load(data_path)\n",
    "    .select(\"content\")\n",
    "    .rdd\n",
    "    .flatMap(lambda x: x[0].splitlines())\n",
    "    .map(functools.partial(parse_line, schema=dataset_schema))\n",
    "    .toDF(create_pyspark_schema(dataset_schema))\n",
    ")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(dbtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED raw.raw_jrdb__BAC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50364985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional helper: filter or log malformed lines that do not match expected total byte span\n",
    "# Expected total bytes (approx) = max(relative + repeat_factor*byte_length - 1) across schema\n",
    "expected_max_end = 0\n",
    "for field in schema_models:\n",
    "    total_len = field.byte_length * field.repeat_factor\n",
    "    end_pos = field.relative - 1 + total_len\n",
    "    expected_max_end = max(expected_max_end, end_pos)\n",
    "\n",
    "print(\"Approx expected record byte length:\", expected_max_end)\n",
    "\n",
    "def is_malformed(line: bytes) -> bool:\n",
    "    return len(line) < expected_max_end\n",
    "\n",
    "malformed = raw_lines_rdd.filter(is_malformed).take(5)\n",
    "if malformed:\n",
    "    print(\"Found malformed line samples (byte lengths):\", [len(m) for m in malformed])\n",
    "else:\n",
    "    print(\"No malformed lines detected in first pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754976e",
   "metadata": {},
   "source": [
    "### Parsing logic explanation\n",
    "We read each BAC*.txt file via the binaryFile source. The `content` column contains the full file as bytes. We `splitlines()` to obtain individual record lines (still bytes), then apply `parse_line`, which:\n",
    "1. Slices fixed-width byte segments using `FieldModel.relative` and `byte_length`.\n",
    "2. Decodes each slice with cp932 and strips whitespace.\n",
    "3. Returns a `Row` whose fields align with the StructType from `create_pyspark_schema` (arrays for repeated fields, strings otherwise).\n",
    "Finally we build `parsed_df` with the explicit schema to avoid inference and ensure array fields are preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7913b",
   "metadata": {},
   "source": [
    "### Note: Fix for PicklingError\n",
    "Passing a list of column names into `spark.createDataFrame(rdd, schema=column_names)` caused Spark to misinterpret the schema argument. A list of strings is not a valid StructType specification, leading to internal serialization (pickling) errors. We now:\n",
    "1. Build a proper `StructType` via `create_pyspark_schema`.\n",
    "2. Map raw lines to `Row` objects with matching arity/order.\n",
    "3. Call `spark.createDataFrame(rdd, schema=struct_schema)`.\n",
    "If only renaming columns were needed, `toDF(*column_names)` would suffice without constructing a StructType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug helper cell: validate schema alignment on a small subset\n",
    "sample = rdd.take(1)\n",
    "print(\"Sample row lens:\", [len(r) for r in sample])\n",
    "print(\"Schema field count:\", len(struct_schema))\n",
    "print(struct_schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e64b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(\n",
    "    spark,\n",
    "    schema_path: str,\n",
    "    data_path: str | List[str],\n",
    "    dbtable: str,\n",
    "    surrogate_key_name: str,\n",
    "):\n",
    "    logger.info(f\"Processing dataset {dbtable}\")\n",
    "    schema = load_schema(schema_path)\n",
    "    logger.info(\"Creating PySpark DataFrame\")\n",
    "    df = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "        .load(data_path)\n",
    "        .select(\"content\")\n",
    "        .rdd.flatMap(lambda x: x[0].splitlines())\n",
    "        .map(functools.partial(parse_line, schema=schema))\n",
    "        .toDF(create_pyspark_schema(schema))\n",
    "        # Todo: monotonic increasing id does not mean files with lower dates will have lower ids!\n",
    "        .withColumn(surrogate_key_name, F.monotonically_increasing_id())\n",
    "        # Returns the wrong file name..\n",
    "        # .withColumn(\"input_file_name\", F.input_file_name())\n",
    "    )\n",
    "    logger.info(\"Writing to data warehouse\")\n",
    "    df.write.mode(\"overwrite\").saveAsTable(dbtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb94f0",
   "metadata": {},
   "source": [
    "The following TYB file in the annual pack contains null byte characters. Its daily file counterpart does not, so we must replace it before the file can be processed.\n",
    "* TYB060121.txt\n",
    "\n",
    "Starting 2021-09-04, TYB files are duplicated in the annual pack. One file name contains a \"_t\" while the other does not. The daily file counterpart contains the same information as the annual pack file whose name does not contain a \"_t\" in it. In addition, some of the \"_t\" files contain null byte characters. The following files are affected. All files with \"_t\" in the name are ignored when parsing.\n",
    "* TYB210904_t.txt\n",
    "* TYB210905_t.txt\n",
    "* TYB210911_t.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62199ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"KAB\",\n",
    "    \"BAC\",\n",
    "    \"KYI\",\n",
    "    \"UKC\",\n",
    "    \"OZ\",\n",
    "    \"OW\",\n",
    "    \"OU\",\n",
    "    \"OT\",\n",
    "    \"OV\",\n",
    "    \"CYB\",\n",
    "    \"CHA\",\n",
    "    \"SKB\",\n",
    "    \"SRB\",\n",
    "    \"HJC\",\n",
    "    \"SED\",  # Run remove_sed_duplicates.sql after loading this dataset\n",
    "    \"TYB\",\n",
    "]\n",
    "\n",
    "schema_name = \"jhra_raw\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    schema_path = f\"schemas/{dataset}.yaml\"\n",
    "    dbtable = f\"{schema_name}.raw_jrdb__{dataset.lower()}\"\n",
    "    surrogate_key_name = f\"{dataset.lower()}_sk\"\n",
    "    # TYB is a special case because the file names are not consistent\n",
    "    if dataset == \"TYB\":\n",
    "        tyb_pattern = re.compile(r\"TYB\\d{6}\\.txt$\")\n",
    "        tyb_files_glob = Path(download_dir).glob(\"TYB*.txt\")\n",
    "        tyb_files = [\n",
    "            str(file) for file in tyb_files_glob if tyb_pattern.match(file.name)\n",
    "        ]\n",
    "        etl(\n",
    "            spark,\n",
    "            schema_path=schema_path,\n",
    "            data_path=tyb_files,\n",
    "            dbtable=dbtable,\n",
    "            surrogate_key_name=surrogate_key_name,\n",
    "        )\n",
    "    else:\n",
    "        etl(\n",
    "            spark,\n",
    "            schema_path=schema_path,\n",
    "            data_path=str(Path(download_dir).joinpath(f\"{dataset}*.txt\")),\n",
    "            dbtable=dbtable,\n",
    "            surrogate_key_name=surrogate_key_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7857759",
   "metadata": {},
   "source": [
    "# Convert codes to CSV format\n",
    "\n",
    "Copy and paste text from the code web pages into the following block, run cell, and save as a CSV file in the `seeds` directory.\n",
    "\n",
    "* [ＪＲＤＢデータコード表](http://www.jrdb.com/program/jrdb_code.txt)\n",
    "* [脚元コード表（2017.02.20）](http://www.jrdb.com/program/ashimoto_code.txt)\n",
    "* [馬具コード表（2017.07.02）](http://www.jrdb.com/program/bagu_code.txt)\n",
    "* [特記コード表（2008.02.23）](http://www.jrdb.com/program/tokki_code.txt)\n",
    "* [系統コード表（2003.05.15）](http://www.jrdb.com/program/keito_code.txt)\n",
    "* [調教コースコード表（2009.10.09）](http://www.jrdb.com/program/cyokyo_course_code.txt)\n",
    "* [追い状態コード表（2008.09.28）](http://www.jrdb.com/program/oi_code.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"\n",
    "01      流す\n",
    "02      余力あり\n",
    "03      終い抑え\n",
    "04      一杯\n",
    "05      バテる\n",
    "06      伸びる\n",
    "07      テンのみ\n",
    "08      鋭く伸び\n",
    "09      強目\n",
    "10      終い重点\n",
    "11      ８分追い\n",
    "12      追って伸\n",
    "13      向正面\n",
    "14      ゲート\n",
    "15      障害練習\n",
    "16      中間軽め\n",
    "17      キリ\n",
    "21      引っ張る\n",
    "22      掛かる\n",
    "23      掛リバテ\n",
    "24      テン掛る\n",
    "25      掛り一杯\n",
    "26      ササル\n",
    "27      ヨレル\n",
    "28      バカつく\n",
    "29      手間取る\n",
    "99      その他\n",
    "\"\"\"\n",
    "\n",
    "result = []\n",
    "for line in code_text.strip().splitlines():\n",
    "    result.append(line.strip().split())\n",
    "\n",
    "print(pd.DataFrame(result).to_csv(index=False, header=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "japanhorseraceanalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
