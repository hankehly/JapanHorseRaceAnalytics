{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0803387-a429-46e9-8804-1d116d719ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import functools\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from jhra.data.data_schema import create_pyspark_schema, load_schema, parse_line\n",
    "\n",
    "# from jhra.data.file_downloader import download_and_extract_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5f359",
   "metadata": {},
   "source": [
    "# Download files from JRDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"JRDB_USERNAME\")\n",
    "password = os.getenv(\"JRDB_PASSWORD\")\n",
    "download_dir = \"path/to/data/jrdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8096d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset_urls = [\n",
    "    # Taken from http://www.jrdb.com/member/dataindex.html\n",
    "    # Comment out the ones you don't want to download.\n",
    "    # Downloading all of them will take about ?\n",
    "    \"http://www.jrdb.com/member/datazip/Kab/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Bac/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Kyi/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Ukc/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Oz/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Oz/index2.html\",  # OW data\n",
    "    # \"http://www.jrdb.com/member/datazip/Ou/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Ot/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Ov/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Cyb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Cha/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Sed/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Skb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Tyb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Hjc/index.html\",\n",
    "]\n",
    "\n",
    "for webpage_url in target_dataset_urls:\n",
    "    download_and_extract_files(\n",
    "        webpage_url, username, password, download_dir, start_date=datetime.date(2023, 12, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d318",
   "metadata": {},
   "source": [
    "# Ingest data into warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05873ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/12 06:05:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(\"spark.sql.warehouse.dir\", os.environ.get(\"SPARK_WAREHOUSE_DIR\"))\n",
    "    .config(\"spark.jars\", os.environ.get(\"POSTGRES_JDBC_JAR\"))\n",
    "    .config(\"spark.executor.extraClassPath\", os.environ.get(\"POSTGRES_JDBC_JAR\"))\n",
    "    .config(\"spark.driver.extraClassPath\", os.environ.get(\"POSTGRES_JDBC_JAR\"))\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset = \"BAC\"\n",
    "dbtable = f\"jhra.raw_jrdb__{dataset.lower()}\"\n",
    "data_path = str(Path(\"/workspace/data/jrdb\").joinpath(f\"{dataset}*.txt\"))\n",
    "dataset_schema = load_schema(\"schemas/BAC.yaml\")\n",
    "column_names = [field.name for field in dataset_schema]\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .load(data_path)\n",
    "    .select(\"content\")\n",
    "    .rdd\n",
    "    .flatMap(lambda x: x[0].splitlines())\n",
    "    .map(functools.partial(parse_line, schema=dataset_schema))\n",
    "    .toDF(create_pyspark_schema(dataset_schema))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c1dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/10/12 06:06:10 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/10/12 06:06:10 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/10/12 06:06:11 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/10/12 06:06:11 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.3\n",
      "25/10/12 06:06:11 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "25/10/12 06:06:12 WARN ObjectStore: Failed to get database jhra, returning NoSuchObjectException\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Database 'jhra' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1041\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/venv/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Database 'jhra' not found"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(dbtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50364985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional helper: filter or log malformed lines that do not match expected total byte span\n",
    "# Expected total bytes (approx) = max(relative + repeat_factor*byte_length - 1) across schema\n",
    "expected_max_end = 0\n",
    "for field in schema_models:\n",
    "    total_len = field.byte_length * field.repeat_factor\n",
    "    end_pos = field.relative - 1 + total_len\n",
    "    expected_max_end = max(expected_max_end, end_pos)\n",
    "\n",
    "print(\"Approx expected record byte length:\", expected_max_end)\n",
    "\n",
    "def is_malformed(line: bytes) -> bool:\n",
    "    return len(line) < expected_max_end\n",
    "\n",
    "malformed = raw_lines_rdd.filter(is_malformed).take(5)\n",
    "if malformed:\n",
    "    print(\"Found malformed line samples (byte lengths):\", [len(m) for m in malformed])\n",
    "else:\n",
    "    print(\"No malformed lines detected in first pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754976e",
   "metadata": {},
   "source": [
    "### Parsing logic explanation\n",
    "We read each BAC*.txt file via the binaryFile source. The `content` column contains the full file as bytes. We `splitlines()` to obtain individual record lines (still bytes), then apply `parse_line`, which:\n",
    "1. Slices fixed-width byte segments using `FieldModel.relative` and `byte_length`.\n",
    "2. Decodes each slice with cp932 and strips whitespace.\n",
    "3. Returns a `Row` whose fields align with the StructType from `create_pyspark_schema` (arrays for repeated fields, strings otherwise).\n",
    "Finally we build `parsed_df` with the explicit schema to avoid inference and ensure array fields are preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7913b",
   "metadata": {},
   "source": [
    "### Note: Fix for PicklingError\n",
    "Passing a list of column names into `spark.createDataFrame(rdd, schema=column_names)` caused Spark to misinterpret the schema argument. A list of strings is not a valid StructType specification, leading to internal serialization (pickling) errors. We now:\n",
    "1. Build a proper `StructType` via `create_pyspark_schema`.\n",
    "2. Map raw lines to `Row` objects with matching arity/order.\n",
    "3. Call `spark.createDataFrame(rdd, schema=struct_schema)`.\n",
    "If only renaming columns were needed, `toDF(*column_names)` would suffice without constructing a StructType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug helper cell: validate schema alignment on a small subset\n",
    "sample = rdd.take(1)\n",
    "print(\"Sample row lens:\", [len(r) for r in sample])\n",
    "print(\"Schema field count:\", len(struct_schema))\n",
    "print(struct_schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e64b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(\n",
    "    spark,\n",
    "    schema_path: str,\n",
    "    data_path: str | List[str],\n",
    "    dbtable: str,\n",
    "    surrogate_key_name: str,\n",
    "):\n",
    "    logger.info(f\"Processing dataset {dbtable}\")\n",
    "    schema = load_schema(schema_path)\n",
    "    logger.info(\"Creating PySpark DataFrame\")\n",
    "    df = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "        .load(data_path)\n",
    "        .select(\"content\")\n",
    "        .rdd.flatMap(lambda x: x[0].splitlines())\n",
    "        .map(functools.partial(parse_line, schema=schema))\n",
    "        .toDF(create_pyspark_schema(schema))\n",
    "        # Todo: monotonic increasing id does not mean files with lower dates will have lower ids!\n",
    "        .withColumn(surrogate_key_name, f.monotonically_increasing_id())\n",
    "        # Returns the wrong file name..\n",
    "        # .withColumn(\"input_file_name\", f.input_file_name())\n",
    "    )\n",
    "    logger.info(\"Writing to data warehouse\")\n",
    "    df.write.mode(\"overwrite\").saveAsTable(dbtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb94f0",
   "metadata": {},
   "source": [
    "The following TYB file in the annual pack contains null byte characters. Its daily file counterpart does not, so we must replace it before the file can be processed.\n",
    "* TYB060121.txt\n",
    "\n",
    "Starting 2021-09-04, TYB files are duplicated in the annual pack. One file name contains a \"_t\" while the other does not. The daily file counterpart contains the same information as the annual pack file whose name does not contain a \"_t\" in it. In addition, some of the \"_t\" files contain null byte characters. The following files are affected. All files with \"_t\" in the name are ignored when parsing.\n",
    "* TYB210904_t.txt\n",
    "* TYB210905_t.txt\n",
    "* TYB210911_t.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62199ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"KAB\",\n",
    "    \"BAC\",\n",
    "    \"KYI\",\n",
    "    \"UKC\",\n",
    "    \"OZ\",\n",
    "    \"OW\",\n",
    "    \"OU\",\n",
    "    \"OT\",\n",
    "    \"OV\",\n",
    "    \"CYB\",\n",
    "    \"CHA\",\n",
    "    \"SKB\",\n",
    "    \"SRB\",\n",
    "    \"HJC\",\n",
    "    \"SED\",  # Run remove_sed_duplicates.sql after loading this dataset\n",
    "    \"TYB\",\n",
    "]\n",
    "\n",
    "schema_name = \"jhra_raw\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    schema_path = f\"schemas/{dataset}.yaml\"\n",
    "    dbtable = f\"{schema_name}.raw_jrdb__{dataset.lower()}\"\n",
    "    surrogate_key_name = f\"{dataset.lower()}_sk\"\n",
    "    # TYB is a special case because the file names are not consistent\n",
    "    if dataset == \"TYB\":\n",
    "        tyb_pattern = re.compile(r\"TYB\\d{6}\\.txt$\")\n",
    "        tyb_files_glob = Path(download_dir).glob(\"TYB*.txt\")\n",
    "        tyb_files = [\n",
    "            str(file) for file in tyb_files_glob if tyb_pattern.match(file.name)\n",
    "        ]\n",
    "        etl(\n",
    "            spark,\n",
    "            schema_path=schema_path,\n",
    "            data_path=tyb_files,\n",
    "            dbtable=dbtable,\n",
    "            surrogate_key_name=surrogate_key_name,\n",
    "        )\n",
    "    else:\n",
    "        etl(\n",
    "            spark,\n",
    "            schema_path=schema_path,\n",
    "            data_path=str(Path(download_dir).joinpath(f\"{dataset}*.txt\")),\n",
    "            dbtable=dbtable,\n",
    "            surrogate_key_name=surrogate_key_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7857759",
   "metadata": {},
   "source": [
    "# Convert codes to CSV format\n",
    "\n",
    "Copy and paste text from the code web pages into the following block, run cell, and save as a CSV file in the `seeds` directory.\n",
    "\n",
    "* [ＪＲＤＢデータコード表](http://www.jrdb.com/program/jrdb_code.txt)\n",
    "* [脚元コード表（2017.02.20）](http://www.jrdb.com/program/ashimoto_code.txt)\n",
    "* [馬具コード表（2017.07.02）](http://www.jrdb.com/program/bagu_code.txt)\n",
    "* [特記コード表（2008.02.23）](http://www.jrdb.com/program/tokki_code.txt)\n",
    "* [系統コード表（2003.05.15）](http://www.jrdb.com/program/keito_code.txt)\n",
    "* [調教コースコード表（2009.10.09）](http://www.jrdb.com/program/cyokyo_course_code.txt)\n",
    "* [追い状態コード表（2008.09.28）](http://www.jrdb.com/program/oi_code.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"\n",
    "01      流す\n",
    "02      余力あり\n",
    "03      終い抑え\n",
    "04      一杯\n",
    "05      バテる\n",
    "06      伸びる\n",
    "07      テンのみ\n",
    "08      鋭く伸び\n",
    "09      強目\n",
    "10      終い重点\n",
    "11      ８分追い\n",
    "12      追って伸\n",
    "13      向正面\n",
    "14      ゲート\n",
    "15      障害練習\n",
    "16      中間軽め\n",
    "17      キリ\n",
    "21      引っ張る\n",
    "22      掛かる\n",
    "23      掛リバテ\n",
    "24      テン掛る\n",
    "25      掛り一杯\n",
    "26      ササル\n",
    "27      ヨレル\n",
    "28      バカつく\n",
    "29      手間取る\n",
    "99      その他\n",
    "\"\"\"\n",
    "\n",
    "result = []\n",
    "for line in code_text.strip().splitlines():\n",
    "    result.append(line.strip().split())\n",
    "\n",
    "print(pd.DataFrame(result).to_csv(index=False, header=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "japanhorseraceanalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
