{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass (3 classes)\n",
    "\n",
    "Try using 3 classes\n",
    "1. 1-3\n",
    "2. 4-8\n",
    "3. 9-\n",
    "\n",
    "* 1レースにつき100円で単勝を買う。\n",
    "* 1レース中にデータが残っている馬数が出走頭数の半分以下の場合は買わない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import japanize_matplotlib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import tqdm\n",
    "import trueskill\n",
    "from hyperopt import STATUS_OK, SparkTrials, Trials, fmin, hp, tpe\n",
    "from hyperopt.pyll.base import scope\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from JapanHorseRaceAnalytics.utilities.base import (\n",
    "    get_random_seed,\n",
    "    get_spark_session,\n",
    "    read_hive_table,\n",
    ")\n",
    "from JapanHorseRaceAnalytics.utilities.metrics import (\n",
    "    calculate_binary_classifier_statistics,\n",
    "    calculate_payout_rate,\n",
    "    kelly_criterion,\n",
    ")\n",
    "from JapanHorseRaceAnalytics.utilities.plot import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_correlation_matrix,\n",
    "    plot_feature_importances,\n",
    "    plot_shap_interaction_values,\n",
    ")\n",
    "from JapanHorseRaceAnalytics.utilities.structured_logger import logger\n",
    "\n",
    "japanize_matplotlib.japanize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 06:55:50 WARN Utils: Your hostname, Hanks-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.40.105 instead (on interface en0)\n",
      "24/03/19 06:55:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/19 06:55:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"Read from hive jhra_curated.features_20240304_v1\", \"level\": \"info\", \"timestamp\": \"2024-03-18T21:55:51.076623Z\", \"logger\": \"JapanHorseRaceAnalytics.utilities.base\"}\n",
      "24/03/19 06:55:51 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/03/19 06:55:51 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/03/19 06:55:52 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "{\"event\": \"Write to parquet /Users/hankehly/Projects/JapanHorseRaceAnalytics/data/sql_tables/features_20240304_v1.snappy.parquet\", \"level\": \"info\", \"timestamp\": \"2024-03-18T21:55:53.274384Z\", \"logger\": \"JapanHorseRaceAnalytics.utilities.base\"}\n",
      "24/03/19 06:55:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "{\"event\": \"Read from parquet /Users/hankehly/Projects/JapanHorseRaceAnalytics/data/sql_tables/features_20240304_v1.snappy.parquet to pandas\", \"level\": \"info\", \"timestamp\": \"2024-03-18T21:55:57.959107Z\", \"logger\": \"JapanHorseRaceAnalytics.utilities.base\"}\n",
      "{\"event\": \"Original data length: 1217019\", \"level\": \"info\", \"timestamp\": \"2024-03-18T21:56:04.873812Z\", \"logger\": \"JapanHorseRaceAnalytics.utilities.base\"}\n",
      "{\"event\": \"Data length after filtering: 1206122 (dropped 10897 rows, 0.90%)\", \"level\": \"info\", \"timestamp\": \"2024-03-18T21:56:05.232691Z\", \"logger\": \"JapanHorseRaceAnalytics.utilities.base\"}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_単勝払戻金</th>\n",
       "      <th>meta_複勝払戻金</th>\n",
       "      <th>meta_レースキー</th>\n",
       "      <th>meta_馬番</th>\n",
       "      <th>meta_血統登録番号</th>\n",
       "      <th>meta_発走日時</th>\n",
       "      <th>meta_単勝的中</th>\n",
       "      <th>meta_単勝オッズ</th>\n",
       "      <th>meta_複勝的中</th>\n",
       "      <th>meta_複勝オッズ</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_6走前休養理由分類コード</th>\n",
       "      <th>num_6走前3着タイム差</th>\n",
       "      <th>cat_トラック種別</th>\n",
       "      <th>num_距離</th>\n",
       "      <th>num_過去3走重み付き着順成績</th>\n",
       "      <th>num_入厩何日前逆数</th>\n",
       "      <th>cat_堅実な馬</th>\n",
       "      <th>cat_過去3走中1走訳あり凡走</th>\n",
       "      <th>cat_過去3走中2走好走</th>\n",
       "      <th>cat_過去3走繋がりあり</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>09025206</td>\n",
       "      <td>11</td>\n",
       "      <td>00100027</td>\n",
       "      <td>2002-12-01 12:45:00+09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>38.6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>芝</td>\n",
       "      <td>1600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>08032303</td>\n",
       "      <td>04</td>\n",
       "      <td>00100027</td>\n",
       "      <td>2003-02-08 10:55:00+09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ダート</td>\n",
       "      <td>1800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>08032801</td>\n",
       "      <td>14</td>\n",
       "      <td>00100027</td>\n",
       "      <td>2003-02-23 10:00:00+09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ダート</td>\n",
       "      <td>1800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>09031403</td>\n",
       "      <td>07</td>\n",
       "      <td>00100027</td>\n",
       "      <td>2003-03-09 10:55:00+09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ダート</td>\n",
       "      <td>1800</td>\n",
       "      <td>0.173651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>09031701</td>\n",
       "      <td>10</td>\n",
       "      <td>00100027</td>\n",
       "      <td>2003-03-22 10:05:00+09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ダート</td>\n",
       "      <td>1800</td>\n",
       "      <td>0.177772</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meta_単勝払戻金  meta_複勝払戻金 meta_レースキー meta_馬番 meta_血統登録番号  \\\n",
       "0           0           0   09025206      11    00100027   \n",
       "1           0           0   08032303      04    00100027   \n",
       "2           0           0   08032801      14    00100027   \n",
       "3           0           0   09031403      07    00100027   \n",
       "4           0           0   09031701      10    00100027   \n",
       "\n",
       "                  meta_発走日時  meta_単勝的中  meta_単勝オッズ  meta_複勝的中  meta_複勝オッズ  \\\n",
       "0 2002-12-01 12:45:00+09:00          0        38.6          0         4.4   \n",
       "1 2003-02-08 10:55:00+09:00          0        39.5          0        11.8   \n",
       "2 2003-02-23 10:00:00+09:00          0        31.0          0         3.7   \n",
       "3 2003-03-09 10:55:00+09:00          0        17.2          0         2.3   \n",
       "4 2003-03-22 10:05:00+09:00          0        20.8          0         2.7   \n",
       "\n",
       "   ...  cat_6走前休養理由分類コード  num_6走前3着タイム差  cat_トラック種別 num_距離 num_過去3走重み付き着順成績  \\\n",
       "0  ...              None            NaN           芝   1600              NaN   \n",
       "1  ...              None            NaN         ダート   1800              NaN   \n",
       "2  ...              None            NaN         ダート   1800              NaN   \n",
       "3  ...              None            NaN         ダート   1800         0.173651   \n",
       "4  ...              None            NaN         ダート   1800         0.177772   \n",
       "\n",
       "   num_入厩何日前逆数  cat_堅実な馬 cat_過去3走中1走訳あり凡走  cat_過去3走中2走好走  cat_過去3走繋がりあり  \n",
       "0          1.0     False            False          False          False  \n",
       "1          1.0     False            False          False          False  \n",
       "2          1.0     False            False          False          False  \n",
       "3          1.0     False            False          False          False  \n",
       "4          1.0     False            False          False          False  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_hive_table(\n",
    "    table_name=\"features_20240304_v1\",\n",
    "    schema=\"jhra_curated\",\n",
    "    spark_session=spark,\n",
    "    use_cache=False,\n",
    "    parse_dates=[\"meta_発走日時\"],\n",
    ")\n",
    "\n",
    "rows_before = data.shape[0]\n",
    "logger.info(f\"Original data length: {rows_before}\")\n",
    "\n",
    "# Drop from data where cat_トラック種別 == \"障害\"\n",
    "# Keep only horses that have 3 races\n",
    "# Keep only data from 2000 onwards\n",
    "data = data[\n",
    "    # (data[\"cat_トラック種別\"] != \"障害\")\n",
    "    (~data[\"meta_着順\"].isna())\n",
    "    # & (data[\"meta_異常区分\"] == \"0\")\n",
    "    # & (data[\"num_1走前着順\"].notnull())\n",
    "    # & (data[\"num_2走前着順\"].notnull())\n",
    "    # & (data[\"num_3走前着順\"].notnull())\n",
    "    # & (data[\"meta_発走日時\"] >= \"2000-01-01\")\n",
    "]\n",
    "\n",
    "rows_after = data.shape[0]\n",
    "logger.info(\n",
    "    f\"Data length after filtering: {rows_after} (dropped {rows_before - rows_after} rows, {100 * (rows_before - rows_after) / rows_before:.2f}%)\"\n",
    ")\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86237/86237 [02:41<00:00, 535.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df_sorted = data.sort_values(by=[\"meta_発走日時\", \"meta_レースキー\"])\n",
    "\n",
    "# Initialize the TrueSkill environment\n",
    "env = trueskill.TrueSkill(draw_probability=0)  # No draws in horse racing\n",
    "\n",
    "# Initialize ratings for all horses\n",
    "horse_ratings = {\n",
    "    horse_id: env.create_rating()\n",
    "    for horse_id in df_sorted[\"meta_血統登録番号\"].unique()\n",
    "}\n",
    "\n",
    "# Placeholder for ratings at each point in time\n",
    "df_sorted[\"rating_post_race\"] = pd.NA\n",
    "\n",
    "# Iterate through races in chronological order\n",
    "for (race_datetime, race_id), race_data in tqdm.tqdm(df_sorted.groupby([\"meta_発走日時\", \"meta_レースキー\"])):\n",
    "    race_results = race_data.sort_values(\"meta_着順\")\n",
    "    horse_ids = race_results[\"meta_血統登録番号\"].tolist()\n",
    "    horse_groups = [[horse_ratings[horse_id]] for horse_id in horse_ids]\n",
    "    ranks = list(range(len(horse_groups)))  # Lower rank number means a better position\n",
    "\n",
    "    # Update ratings based on the race outcome\n",
    "    updated_ratings = env.rate(horse_groups, ranks=ranks)\n",
    "\n",
    "    # Directly update the DataFrame with the new ratings\n",
    "    for index, (horse_id, new_rating_group) in zip(race_results.index, zip(horse_ids, updated_ratings)):\n",
    "        horse_ratings[horse_id] = new_rating_group[0]  # Update with new rating\n",
    "        df_sorted.at[index, \"rating_post_race\"] = horse_ratings[horse_id].mu  # Directly assign the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag the rating_post_race by one row per horse (meta_血統登録番号) chronologically (meta_発走日時)\n",
    "df_sorted[\"rating_pre_race\"] = (\n",
    "    df_sorted.sort_values(\"meta_発走日時\")\n",
    "    .groupby(\"meta_血統登録番号\")[\"rating_post_race\"]\n",
    "    .shift(1)\n",
    "    .fillna(25.0)\n",
    ")\n",
    "\n",
    "# Step 1: Calculate the total rating sum for each race\n",
    "race_rating_sum = df_sorted.groupby(\"meta_レースキー\")[\"rating_pre_race\"].transform(\"sum\")\n",
    "\n",
    "# Step 2: Calculate the count of horses in each race\n",
    "race_horse_count = df_sorted.groupby(\"meta_レースキー\")[\"rating_pre_race\"].transform(\"count\")\n",
    "\n",
    "# Step 3: Calculate the mean competitor rating for each horse\n",
    "df_sorted[\"mean_competitor_rating_pre_race\"] = (race_rating_sum - df_sorted[\"rating_pre_race\"]) / (race_horse_count - 1)\n",
    "\n",
    "# Step 4: Get the diff between the horse's rating and the mean competitor rating\n",
    "df_sorted[\"mean_competitor_rating_pre_race_diff\"] = df_sorted[\"rating_pre_race\"] - df_sorted[\"mean_competitor_rating_pre_race\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted[\"num_1走前標準化着順\"] = (df_sorted[\"num_1走前着順\"] - 1) / (df_sorted[\"num_1走前頭数\"] - 1)\n",
    "df_sorted[\"num_2走前標準化着順\"] = (df_sorted[\"num_2走前着順\"] - 1) / (df_sorted[\"num_2走前頭数\"] - 1)\n",
    "df_sorted[\"num_3走前標準化着順\"] = (df_sorted[\"num_3走前着順\"] - 1) / (df_sorted[\"num_3走前頭数\"] - 1)\n",
    "\n",
    "# We want our factor (num_1走前経過日数) to start from 0\n",
    "# so subtract the minimum value for \"days since last race\" across all horses\n",
    "# Add a small number to avoid division by zero\n",
    "df_sorted[\"num_1走前重み\"] = 1 / (df_sorted[\"num_1走前経過日数\"] - df_sorted[\"num_1走前経過日数\"].min() + 1e-6)\n",
    "df_sorted[\"num_2走前重み\"] = 1 / (df_sorted[\"num_2走前経過日数\"] - df_sorted[\"num_1走前経過日数\"].min() + 1e-6)\n",
    "df_sorted[\"num_3走前重み\"] = 1 / (df_sorted[\"num_3走前経過日数\"] - df_sorted[\"num_1走前経過日数\"].min() + 1e-6)\n",
    "\n",
    "# Calculate weighted average of the feature\n",
    "df_sorted[\"num_過去3走重み付き標準化着順\"] = (\n",
    "    (df_sorted[\"num_1走前標準化着順\"] * df_sorted[\"num_1走前重み\"])\n",
    "    + (df_sorted[\"num_2走前標準化着順\"] * df_sorted[\"num_2走前重み\"])\n",
    "    + (df_sorted[\"num_3走前標準化着順\"] * df_sorted[\"num_3走前重み\"])\n",
    ") / (df_sorted[\"num_1走前重み\"] + df_sorted[\"num_2走前重み\"] + df_sorted[\"num_3走前重み\"])\n",
    "\n",
    "# 2. Weighted average time difference between the horse and the 3 horses behind it\n",
    "df_sorted[\"num_1走前後続馬平均タイム差\"] = (\n",
    "    df_sorted[[\"num_1走前後続馬1タイム差\", \"num_1走前後続馬2タイム差\", \"num_1走前後続馬3タイム差\"]]\n",
    "    .mean(axis=1)\n",
    "    .fillna(0)\n",
    ")\n",
    "df_sorted[\"num_2走前後続馬平均タイム差\"] = (\n",
    "    df_sorted[[\"num_2走前後続馬1タイム差\", \"num_2走前後続馬2タイム差\", \"num_2走前後続馬3タイム差\"]]\n",
    "    .mean(axis=1)\n",
    "    .fillna(0)\n",
    ")\n",
    "df_sorted[\"num_3走前後続馬平均タイム差\"] = (\n",
    "    df_sorted[[\"num_3走前後続馬1タイム差\", \"num_3走前後続馬2タイム差\", \"num_3走前後続馬3タイム差\"]]\n",
    "    .mean(axis=1)\n",
    "    .fillna(0)\n",
    ")\n",
    "df_sorted[\"num_過去3走重み付き後続馬平均タイム差\"] = (\n",
    "    (df_sorted[\"num_1走前後続馬平均タイム差\"] * df_sorted[\"num_1走前重み\"])\n",
    "    + (df_sorted[\"num_2走前後続馬平均タイム差\"] * df_sorted[\"num_2走前重み\"])\n",
    "    + (df_sorted[\"num_3走前後続馬平均タイム差\"] * df_sorted[\"num_3走前重み\"])\n",
    ") / (df_sorted[\"num_1走前重み\"] + df_sorted[\"num_2走前重み\"] + df_sorted[\"num_3走前重み\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_sorted[\n",
    "    (df_sorted[\"cat_トラック種別\"] != \"障害\")\n",
    "    & (df_sorted[\"num_1走前着順\"].notnull())\n",
    "    & (df_sorted[\"num_2走前着順\"].notnull())\n",
    "    & (df_sorted[\"num_3走前着順\"].notnull())\n",
    "]\n",
    "\n",
    "# Remove rows where number of records for meta_レースキー is less than num_頭数 // 2\n",
    "df_final = df_final[\n",
    "    df_final.groupby(\"meta_レースキー\")[\"meta_レースキー\"].transform(\"count\")\n",
    "    > df_final[\"num_頭数\"] // 2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (610942, 124)\n",
      "X_test: (152462, 124)\n",
      "y_train: (610942,)\n",
      "y_test: (152462,)\n"
     ]
    }
   ],
   "source": [
    "def make_target(x):\n",
    "    \"\"\"\n",
    "    https://qiita.com/Mshimia/items/6c54d82b3792925b8199#%E4%BA%88%E6%B8%AC%E3%83%A2%E3%83%87%E3%83%AB\n",
    "    \"\"\"\n",
    "    if x <= 3:\n",
    "        return 0\n",
    "    elif x <= 8:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=get_random_seed())\n",
    "\n",
    "for train_idx, test_idx in gss.split(df_final, groups=df_final[\"meta_レースキー\"]):\n",
    "    train_df = df_final.iloc[train_idx]\n",
    "    test_df = df_final.iloc[test_idx]\n",
    "\n",
    "# train_df and test_df contain your split data, respecting group boundaries\n",
    "X_train = train_df.copy()\n",
    "y_train = X_train[\"meta_着順\"].apply(make_target)\n",
    "\n",
    "X_test = test_df.copy()\n",
    "y_test = X_test[\"meta_着順\"].apply(make_target)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective_fn(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    mlflow_experiment_name: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    payouts should have the same index as *_test and have the following columns:\n",
    "    * 発走日時\n",
    "    * odds\n",
    "    * payout\n",
    "    \"\"\"\n",
    "\n",
    "    def train(params):\n",
    "        mlflow.set_experiment(experiment_name=mlflow_experiment_name)\n",
    "        with mlflow.start_run():\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\n",
    "                        \"num\",\n",
    "                        StandardScaler(),\n",
    "                        [\n",
    "                            \"num_複勝率\",\n",
    "                            \"num_1走前経過日数\",\n",
    "                            \"num_過去3走重み付き標準化着順\",\n",
    "                            \"num_過去3走重み付き後続馬平均タイム差\",\n",
    "                            \"mean_competitor_rating_pre_race\",\n",
    "                            \"rating_pre_race\",\n",
    "                        ],\n",
    "                    ),\n",
    "                ],\n",
    "                remainder=\"drop\",\n",
    "            )\n",
    "\n",
    "            X_train_prep = preprocessor.fit_transform(X_train, y_train)\n",
    "            X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "            # Apply SMOTE\n",
    "            smote_params = {\n",
    "                k.split(\"__\")[1]: v\n",
    "                for k, v in params.items()\n",
    "                if k.startswith(\"smote__\")\n",
    "            }\n",
    "            smote = SMOTE(**smote_params)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(\n",
    "                X_train_prep, y_train\n",
    "            )\n",
    "\n",
    "            train_set = lgb.Dataset(X_train_resampled, label=y_train_resampled)\n",
    "            valid_set = lgb.Dataset(X_test_prep, label=y_test, reference=train_set)\n",
    "\n",
    "            classifier_params = {\n",
    "                k.split(\"__\")[1]: v\n",
    "                for k, v in params.items()\n",
    "                if k.startswith(\"classifier__\")\n",
    "            }\n",
    "            model = lgb.train(\n",
    "                params=classifier_params,\n",
    "                train_set=train_set,\n",
    "                valid_sets=[valid_set],\n",
    "            )\n",
    "\n",
    "            # Predict the class for each data point\n",
    "            y_pred = np.argmax(model.predict(X_test_prep), axis=1)\n",
    "\n",
    "            # If you want the probabilities to calculate log loss\n",
    "            y_pred_proba = model.predict(X_test_prep)\n",
    "\n",
    "            metrics = {\n",
    "                \"loss\": -precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "                \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "                \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "                \"f1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "                \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "            }\n",
    "\n",
    "            mlflow.log_params(classifier_params)\n",
    "            mlflow.log_metrics(metrics)\n",
    "\n",
    "            # Suppress UserWarning messages from matplotlib\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "            # Confusion Matrix\n",
    "            fig, _ = plot_confusion_matrix(y_test, y_pred)\n",
    "            with tempfile.NamedTemporaryFile(\n",
    "                prefix=\"confusion_matrix_\", suffix=\".png\"\n",
    "            ) as f:\n",
    "                fig.savefig(f.name)\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(f.name)\n",
    "\n",
    "            # Feature Importances Plot (Gain)\n",
    "            fig, ax = plot_feature_importances(\n",
    "                feature_names=preprocessor.get_feature_names_out(),\n",
    "                feature_importances=model.feature_importance(importance_type=\"gain\"),\n",
    "                top_n=50,\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(\n",
    "                prefix=\"feature_importance_gain_\", suffix=\".png\"\n",
    "            ) as f:\n",
    "                fig.savefig(f.name)\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(f.name)\n",
    "\n",
    "            # Feature Importances Data (Gain)\n",
    "            feature_importances = zip(\n",
    "                preprocessor.get_feature_names_out(),\n",
    "                model.feature_importance(importance_type=\"gain\"),\n",
    "            )\n",
    "            feature_importances_df = (\n",
    "                pd.DataFrame(feature_importances, columns=[\"feature\", \"importance\"])\n",
    "                .sort_values(\"importance\", ascending=False)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(\n",
    "                prefix=\"feature_importance_gain_\", suffix=\".csv\"\n",
    "            ) as f:\n",
    "                feature_importances_df.to_csv(f.name, index=False)\n",
    "                mlflow.log_artifact(f.name)\n",
    "\n",
    "            # Feature Importances Plot (Split)\n",
    "            fig, ax = plot_feature_importances(\n",
    "                feature_names=preprocessor.get_feature_names_out(),\n",
    "                feature_importances=model.feature_importance(importance_type=\"split\"),\n",
    "                top_n=50,\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(\n",
    "                prefix=\"feature_importance_split_\", suffix=\".png\"\n",
    "            ) as f:\n",
    "                fig.savefig(f.name)\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(f.name)\n",
    "\n",
    "            # Feature Importances Data (Split)\n",
    "            feature_importances = zip(\n",
    "                preprocessor.get_feature_names_out(),\n",
    "                model.feature_importance(importance_type=\"split\"),\n",
    "            )\n",
    "            feature_importances_df = (\n",
    "                pd.DataFrame(feature_importances, columns=[\"feature\", \"importance\"])\n",
    "                .sort_values(\"importance\", ascending=False)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(\n",
    "                prefix=\"feature_importance_split_\", suffix=\".csv\"\n",
    "            ) as f:\n",
    "                feature_importances_df.to_csv(f.name, index=False)\n",
    "                mlflow.log_artifact(f.name)\n",
    "\n",
    "            # SHAP values\n",
    "            X_test_sample = X_test.sample(n=10_000, random_state=get_random_seed())\n",
    "            X_test_sample_prep = preprocessor.transform(X_test_sample)\n",
    "            explainer = shap.TreeExplainer(\n",
    "                model=model,\n",
    "                feature_names=preprocessor.get_feature_names_out(),\n",
    "            )\n",
    "            # 0 is the class index for True\n",
    "            shap_values = explainer(X_test_sample_prep)[:, :, 0]\n",
    "            shap_interaction_values = explainer.shap_interaction_values(\n",
    "                X_test_sample_prep\n",
    "            )\n",
    "\n",
    "            # SHAP beeswarm plot\n",
    "            shap.plots.beeswarm(shap_values, show=False)\n",
    "            plt.tight_layout()\n",
    "            with tempfile.NamedTemporaryFile(\n",
    "                prefix=\"shap_beeswarm_\", suffix=\".png\"\n",
    "            ) as f:\n",
    "                plt.savefig(f.name)\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(f.name)\n",
    "\n",
    "            # SHAP interaction values heatmap\n",
    "            fig, ax = plot_shap_interaction_values(\n",
    "                shap_interaction_values, preprocessor.get_feature_names_out()\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(\n",
    "                prefix=\"shap_interactions_\", suffix=\".png\"\n",
    "            ) as f:\n",
    "                fig.savefig(f.name)\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(f.name)\n",
    "\n",
    "            # SHAP bar plot\n",
    "            shap.plots.bar(shap_values, show=False)\n",
    "            plt.tight_layout()\n",
    "            with tempfile.NamedTemporaryFile(prefix=\"shap_bar_\", suffix=\".png\") as f:\n",
    "                plt.savefig(f.name)\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(f.name)\n",
    "\n",
    "            # Correlation matrix\n",
    "            fig, ax = plot_correlation_matrix(\n",
    "                data=preprocessor.transform(X_test),\n",
    "                columns=preprocessor.get_feature_names_out(),\n",
    "            )\n",
    "            with tempfile.NamedTemporaryFile(\n",
    "                prefix=\"correlation_matrix_\", suffix=\".png\"\n",
    "            ) as f:\n",
    "                fig.savefig(f.name)\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(f.name)\n",
    "            return {\"status\": STATUS_OK, \"params\": params, \"model\": model, **metrics}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    # Prefix SMOTE parameters with \"smote__\"\n",
    "    \"smote__random_state\": get_random_seed(),\n",
    "    # Prefix classifier parameters with \"classifier__\"\n",
    "    \"classifier__num_leaves\": scope.int(hp.quniform(\"num_leaves\", 20, 150, 1)),\n",
    "    \"classifier__max_depth\": scope.int(hp.quniform(\"max_depth\", 3, 10, 1)),\n",
    "    \"classifier__learning_rate\": hp.loguniform(\"learning_rate\", -5, 0),\n",
    "    \"classifier__n_estimators\": scope.int(hp.quniform(\"n_estimators\", 100, 1000, 1)),\n",
    "    \"classifier__min_child_samples\": scope.int(\n",
    "        hp.quniform(\"min_child_samples\", 20, 500, 1)\n",
    "    ),\n",
    "    \"classifier__subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    \"classifier__colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "    \"classifier__reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 1.0),\n",
    "    \"classifier__reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "    \"classifier__feature_fraction\": hp.uniform(\"feature_fraction\", 0.5, 1.0),\n",
    "    \"classifier__lambda_l1\": hp.uniform(\"lambda_l1\", 0, 5),\n",
    "    \"classifier__lambda_l2\": hp.uniform(\"lambda_l2\", 0, 5),\n",
    "    \"classifier__min_split_gain\": hp.uniform(\"min_split_gain\", 0, 1),\n",
    "    \"classifier__min_child_weight\": hp.uniform(\"min_child_weight\", 0.001, 10),\n",
    "    \"classifier__boosting_type\": \"gbdt\",\n",
    "    \"classifier__objective\": \"multiclassova\",\n",
    "    \"classifier__num_class\": 3,\n",
    "    \"classifier__metric\": \"multi_error\",\n",
    "    \"classifier__verbose\": -1,\n",
    "    \"classifier__random_state\": get_random_seed(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"20240318_eda2_3class\"\n",
    "\n",
    "if mlflow.get_experiment_by_name(experiment_name) is None:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "\n",
    "fn = create_objective_fn(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    mlflow_experiment_name=experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6200128394071927,\n",
       " 'feature_fraction': 0.6369370526744543,\n",
       " 'lambda_l1': 2.974353146652003,\n",
       " 'lambda_l2': 0.03017023287643028,\n",
       " 'learning_rate': 0.04177858173712023,\n",
       " 'max_depth': 5.0,\n",
       " 'min_child_samples': 238.0,\n",
       " 'min_child_weight': 7.497597144935071,\n",
       " 'min_split_gain': 0.07812159082885184,\n",
       " 'n_estimators': 972.0,\n",
       " 'num_leaves': 143.0,\n",
       " 'reg_alpha': 0.14861641212382748,\n",
       " 'reg_lambda': 0.6538265648271644,\n",
       " 'subsample': 0.7626081055867588}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trials = Trials()\n",
    "# fmin(\n",
    "#     fn=fn,\n",
    "#     space=space,\n",
    "#     algo=tpe.suggest,\n",
    "#     max_evals=1,\n",
    "#     trials=trials,\n",
    "# )\n",
    "\n",
    "trials = SparkTrials(parallelism=3, spark_session=spark)\n",
    "best = fmin(fn=fn, space=space, algo=tpe.suggest, max_evals=30, trials=trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found `n_estimators` in params. Will use it instead of argument\n",
      "The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            [\n",
    "                \"num_複勝率\",\n",
    "                \"num_1走前経過日数\",\n",
    "                \"num_過去3走重み付き標準化着順\",\n",
    "                \"num_過去3走重み付き後続馬平均タイム差\",\n",
    "                \"mean_competitor_rating_pre_race\",\n",
    "                \"mean_competitor_rating_pre_race_diff\",\n",
    "                \"rating_pre_race\",\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "X_train_prep = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=get_random_seed())\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_prep, y_train)\n",
    "\n",
    "train_set = lgb.Dataset(X_train_resampled, label=y_train_resampled)\n",
    "valid_set = lgb.Dataset(X_test_prep, label=y_test, reference=train_set)\n",
    "\n",
    "classifier_params = {\n",
    "    \"colsample_bytree\": 0.6200128394071927,\n",
    "    \"feature_fraction\": 0.6369370526744543,\n",
    "    \"lambda_l1\": 2.974353146652003,\n",
    "    \"lambda_l2\": 0.03017023287643028,\n",
    "    \"learning_rate\": 0.04177858173712023,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_child_samples\": 238,\n",
    "    \"min_child_weight\": 7.497597144935071,\n",
    "    \"min_split_gain\": 0.07812159082885184,\n",
    "    \"n_estimators\": 972,\n",
    "    \"num_leaves\": 143,\n",
    "    \"reg_alpha\": 0.14861641212382748,\n",
    "    \"reg_lambda\": 0.6538265648271644,\n",
    "    \"subsample\": 0.7626081055867588,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclassova\",\n",
    "    \"num_class\": 3,\n",
    "    \"metric\": \"multi_error\",\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": get_random_seed(),\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params=classifier_params,\n",
    "    train_set=train_set,\n",
    "    valid_sets=[valid_set],\n",
    ")\n",
    "\n",
    "# Predict the class for each data point\n",
    "y_pred = np.argmax(model.predict(X_test_prep), axis=1)\n",
    "\n",
    "# If you want the probabilities to calculate log loss\n",
    "y_pred_proba = model.predict(X_test_prep)\n",
    "\n",
    "metrics = {\n",
    "    \"loss\": -precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "    \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "    \"f1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "    \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "payouts = (\n",
    "    X_test[[\"meta_レースキー\", \"meta_血統登録番号\", \"meta_着順\", \"meta_発走日時\", \"meta_単勝オッズ\", \"meta_複勝オッズ\"]]\n",
    "    .assign(meta_発走日時=lambda x: pd.to_datetime(x[\"meta_発走日時\"]))\n",
    "    .assign(y_test=y_test, y_pred=y_pred, y_pred_proba_0=y_pred_proba[:, 0])\n",
    ")\n",
    "\n",
    "bets = []\n",
    "for i, (race_key, group) in enumerate(payouts.groupby(\"meta_レースキー\")):\n",
    "    top_horse = group.sort_values(\"y_pred_proba_0\", ascending=False).iloc[0]\n",
    "    if top_horse.y_pred == 0:\n",
    "        bets.append([race_key, top_horse[\"meta_血統登録番号\"], True])\n",
    "\n",
    "payouts = payouts.merge(\n",
    "    pd.DataFrame(bets, columns=[\"meta_レースキー\", \"meta_血統登録番号\", \"bet\"]),\n",
    "    on=[\"meta_レースキー\", \"meta_血統登録番号\"],\n",
    "    how=\"left\",\n",
    ").fillna({\"bet\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "payout_t   -226170.0\n",
       "payout_f   -283620.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    payouts\n",
    "    .assign(hit_f=lambda x: x[\"bet\"] & (x[\"meta_着順\"] <= 3))\n",
    "    .assign(miss_f=lambda x: x[\"bet\"] & ~x[\"hit_f\"])\n",
    "    .assign(payout_f=lambda x: x[\"hit_f\"] * (x[\"meta_複勝オッズ\"] * 100 - 100) - x[\"miss_f\"] * 100)\n",
    "\n",
    "    .assign(hit_t=lambda x: x[\"bet\"] & (x[\"meta_着順\"] == 1))\n",
    "    .assign(miss_t=lambda x: x[\"bet\"] & ~x[\"hit_t\"])\n",
    "    .assign(payout_t=lambda x: x[\"hit_t\"] * (x[\"meta_単勝オッズ\"] * 100 - 100) - x[\"miss_t\"] * 100)\n",
    "\n",
    "    # .reset_index(drop=True)\n",
    "    .sort_values([\"meta_発走日時\", \"meta_レースキー\"])\n",
    "    # .dropna(subset=[\"meta_単勝オッズ\", \"meta_複勝オッズ\"])\n",
    ")[[\"payout_t\", \"payout_f\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bankroll over time\n",
    "# results = (\n",
    "#     pd.concat(\n",
    "#         [\n",
    "#             payouts,\n",
    "#             pd.Series(y_pred.astype(bool)).rename(\"pred\"),\n",
    "#             pd.Series(y_proba[:, 0]).rename(\"proba_true\"),\n",
    "#             y_test.astype(bool).reset_index(drop=True).rename(\"actual\"),\n",
    "#         ],\n",
    "#         axis=1,\n",
    "#     )\n",
    "#     .set_index(\"発走日時\")\n",
    "#     .sort_index()\n",
    "#     .dropna()\n",
    "# )\n",
    "# b = results[\"odds\"] - 1\n",
    "# p = results[\"proba_true\"]\n",
    "# q = 1 - p\n",
    "# japanize_matplotlib.japanize()\n",
    "# for confidence in [0.5, 0.65, 0.8]:\n",
    "#     fig, ax = plt.subplots(figsize=(15, 5))\n",
    "#     for multiplier in [0.1, 0.2, 0.3]:\n",
    "#         results[f\"kelly_{multiplier}\"] = (\n",
    "#             kelly_criterion(b, p, q).clip(lower=0) * multiplier\n",
    "#         )\n",
    "#         bankroll = 10_000\n",
    "#         history = []\n",
    "#         for i, row in results.iterrows():\n",
    "#             # bet in 100 yen increments\n",
    "#             bet_amount = (\n",
    "#                 round(row[f\"kelly_{multiplier}\"] * bankroll / 100) * 100\n",
    "#             )\n",
    "#             bet = bet_amount > 0 and row[\"proba_true\"] >= confidence\n",
    "#             if bet and row[\"actual\"] is True:\n",
    "#                 bankroll += (row[\"odds\"] - 1) * bet_amount\n",
    "#             elif bet and row[\"actual\"] is False:\n",
    "#                 bankroll -= bet_amount\n",
    "#             history.append(bankroll)\n",
    "#         results[\"bankroll\"] = history\n",
    "#         results[\"bankroll\"].plot(\n",
    "#             ax=ax, label=f\"Kelly Criterion x {multiplier}\"\n",
    "#         )\n",
    "#     ax.set_title(\"Bankroll over time (Confidence: {confidence})\")\n",
    "#     ax.set_ylabel(\"Bankroll\")\n",
    "#     ax.set_xlabel(\"Date\")\n",
    "#     ax.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.grid()\n",
    "#     with tempfile.NamedTemporaryFile(\n",
    "#         prefix=f\"bets_{confidence}_confidence_\", suffix=\".png\"\n",
    "#     ) as f:\n",
    "#         fig.savefig(f.name)\n",
    "#         plt.close()\n",
    "#         mlflow.log_artifact(f.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
